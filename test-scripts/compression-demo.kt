#!/usr/bin/env kotlin

/**
 * ä¸Šä¸‹æ–‡å‹ç¼©åŠŸèƒ½æ¼”ç¤ºè„šæœ¬
 * 
 * æœ¬è„šæœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ mpp-core çš„ä¸Šä¸‹æ–‡å‹ç¼©åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
 * 1. é…ç½®å‹ç¼©å‚æ•°
 * 2. æ¨¡æ‹Ÿé•¿å¯¹è¯åœºæ™¯
 * 3. è§¦å‘è‡ªåŠ¨å‹ç¼©
 * 4. ç›‘æ§å‹ç¼©è¿‡ç¨‹
 */

@file:DependsOn("org.jetbrains.kotlinx:kotlinx-coroutines-core:1.7.3")

import cc.unitmesh.llm.LLMService
import cc.unitmesh.llm.ModelConfig
import cc.unitmesh.llm.LLMProviderType
import cc.unitmesh.llm.compression.CompressionConfig
import cc.unitmesh.llm.compression.CompressionStatus
import cc.unitmesh.agent.conversation.ConversationManager
import cc.unitmesh.devins.llm.Message
import cc.unitmesh.devins.llm.MessageRole
import kotlinx.coroutines.runBlocking

fun main() = runBlocking {
    println("ğŸš€ ä¸Šä¸‹æ–‡å‹ç¼©åŠŸèƒ½æ¼”ç¤º")
    println("=" * 50)
    
    // 1. é…ç½®æ¨¡å‹å’Œå‹ç¼©å‚æ•°
    println("\nğŸ“‹ 1. é…ç½®æ¨¡å‹å’Œå‹ç¼©å‚æ•°")
    
    val modelConfig = ModelConfig(
        provider = LLMProviderType.OPENAI,
        modelName = "gpt-3.5-turbo",
        apiKey = System.getenv("OPENAI_API_KEY") ?: "demo-key",
        baseUrl = "https://api.openai.com/v1",
        maxTokens = 2000  // è®¾ç½®è¾ƒå°çš„é™åˆ¶ä»¥ä¾¿æ¼”ç¤ºå‹ç¼©
    )
    
    val compressionConfig = CompressionConfig(
        contextPercentageThreshold = 0.6,  // 60% æ—¶è§¦å‘å‹ç¼©
        preserveRecentRatio = 0.4,         // ä¿ç•™ 40% çš„æœ€è¿‘å¯¹è¯
        autoCompressionEnabled = true,
        retryAfterMessages = 3
    )
    
    println("   æ¨¡å‹: ${modelConfig.modelName}")
    println("   æœ€å¤§ tokens: ${modelConfig.maxTokens}")
    println("   å‹ç¼©é˜ˆå€¼: ${(compressionConfig.contextPercentageThreshold * 100).toInt()}%")
    println("   ä¿ç•™æ¯”ä¾‹: ${(compressionConfig.preserveRecentRatio * 100).toInt()}%")
    
    // 2. åˆ›å»º LLM æœåŠ¡å’Œå¯¹è¯ç®¡ç†å™¨
    println("\nğŸ”§ 2. åˆå§‹åŒ–æœåŠ¡")
    
    val llmService = LLMService.create(modelConfig, compressionConfig)
    val conversationManager = ConversationManager(
        llmService = llmService,
        systemPrompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·è§£å†³æŠ€æœ¯é—®é¢˜ã€‚",
        autoCompress = true
    )
    
    // 3. è®¾ç½®ç›‘æ§å›è°ƒ
    println("\nğŸ“Š 3. è®¾ç½®ç›‘æ§å›è°ƒ")
    
    conversationManager.onTokenUpdate = { tokenInfo ->
        val usage = tokenInfo.getUsagePercentage(llmService.getMaxTokens())
        println("   ğŸ“ˆ Token ä½¿ç”¨ç‡: ${String.format("%.1f", usage)}%")
        println("      è¾“å…¥: ${tokenInfo.inputTokens}, è¾“å‡º: ${tokenInfo.outputTokens}")
    }
    
    conversationManager.onCompressionNeeded = { currentTokens, maxTokens ->
        val percentage = (currentTokens.toDouble() / maxTokens.toDouble() * 100).toInt()
        println("   âš ï¸  éœ€è¦å‹ç¼©: ${currentTokens}/${maxTokens} (${percentage}%)")
    }
    
    conversationManager.onCompressionCompleted = { result ->
        println("   âœ… å‹ç¼©å®Œæˆ:")
        println("      çŠ¶æ€: ${result.info.compressionStatus}")
        println("      åŸå§‹ tokens: ${result.info.originalTokenCount}")
        println("      å‹ç¼©å tokens: ${result.info.newTokenCount}")
        println("      å‹ç¼©æ¯”ä¾‹: ${String.format("%.1f", result.info.compressionRatio * 100)}%")
        println("      èŠ‚çœ tokens: ${result.info.tokensSaved}")
    }
    
    println("   å›è°ƒè®¾ç½®å®Œæˆ")
    
    // 4. æ¨¡æ‹Ÿé•¿å¯¹è¯åœºæ™¯
    println("\nğŸ’¬ 4. æ¨¡æ‹Ÿé•¿å¯¹è¯åœºæ™¯")
    
    val longConversationTopics = listOf(
        "å¦‚ä½•åœ¨ Kotlin ä¸­å®ç°å•ä¾‹æ¨¡å¼ï¼Ÿ",
        "è§£é‡Šä¸€ä¸‹åç¨‹çš„å·¥ä½œåŸç†",
        "ä»€ä¹ˆæ˜¯ä¾èµ–æ³¨å…¥ï¼Ÿæœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ",
        "å¦‚ä½•ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Ÿ",
        "è§£é‡Š SOLID åŸåˆ™",
        "ä»€ä¹ˆæ˜¯å¾®æœåŠ¡æ¶æ„ï¼Ÿ",
        "å¦‚ä½•å¤„ç†å¹¶å‘ç¼–ç¨‹ä¸­çš„ç«æ€æ¡ä»¶ï¼Ÿ",
        "è§£é‡Š RESTful API çš„è®¾è®¡åŸåˆ™",
        "ä»€ä¹ˆæ˜¯è®¾è®¡æ¨¡å¼ï¼Ÿä¸¾å‡ ä¸ªä¾‹å­",
        "å¦‚ä½•è¿›è¡Œå•å…ƒæµ‹è¯•ï¼Ÿ"
    )
    
    println("   å¼€å§‹æ¨¡æ‹Ÿå¯¹è¯...")
    
    longConversationTopics.forEachIndexed { index, topic ->
        println("\n   ğŸ’­ å¯¹è¯ ${index + 1}: $topic")
        
        // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        conversationManager.addUserMessage(topic)
        
        // æ¨¡æ‹ŸåŠ©æ‰‹å›å¤ï¼ˆå®é™…åœºæ™¯ä¸­è¿™ä¼šæ˜¯ LLM çš„å›å¤ï¼‰
        val mockResponse = generateMockResponse(topic, index)
        conversationManager.addAssistantMessage(mockResponse)
        
        // æ˜¾ç¤ºå½“å‰ç»Ÿè®¡
        val stats = conversationManager.getConversationStats()
        println("      æ¶ˆæ¯æ•°: ${stats.messageCount}")
        println("      ä½¿ç”¨ç‡: ${String.format("%.1f", stats.utilizationRatio * 100)}%")
        
        // æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
        if (conversationManager.needsCompression()) {
            println("      ğŸ”„ è§¦å‘è‡ªåŠ¨å‹ç¼©...")
            val result = conversationManager.compressHistory()
            
            if (result.info.compressionStatus == CompressionStatus.COMPRESSED) {
                println("      âœ… å‹ç¼©æˆåŠŸ")
            } else {
                println("      âŒ å‹ç¼©å¤±è´¥: ${result.info.errorMessage ?: "æœªçŸ¥é”™è¯¯"}")
            }
        }
        
        Thread.sleep(100) // æ¨¡æ‹Ÿå¯¹è¯é—´éš”
    }
    
    // 5. æœ€ç»ˆç»Ÿè®¡
    println("\nğŸ“ˆ 5. æœ€ç»ˆç»Ÿè®¡")
    val finalStats = conversationManager.getConversationStats()
    println("   æ€»æ¶ˆæ¯æ•°: ${finalStats.messageCount}")
    println("   æœ€ç»ˆä½¿ç”¨ç‡: ${String.format("%.1f", finalStats.utilizationRatio * 100)}%")
    println("   Token ä¿¡æ¯: ${finalStats.tokenInfo}")
    
    println("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
}

fun generateMockResponse(topic: String, index: Int): String {
    return """
    è¿™æ˜¯å¯¹"$topic"çš„è¯¦ç»†å›ç­” (ç¬¬${index + 1}è½®å¯¹è¯)ã€‚
    
    åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šåŒ…å«ï¼š
    - è¯¦ç»†çš„æŠ€æœ¯è§£é‡Š
    - ä»£ç ç¤ºä¾‹
    - æœ€ä½³å®è·µå»ºè®®
    - ç›¸å…³èµ„æºé“¾æ¥
    
    è¿™ä¸ªå›ç­”åŒ…å«äº†è¶³å¤Ÿçš„å†…å®¹æ¥æ¨¡æ‹ŸçœŸå®çš„ LLM å“åº”ï¼Œ
    ç”¨äºæµ‹è¯•ä¸Šä¸‹æ–‡å‹ç¼©åŠŸèƒ½çš„è§¦å‘å’Œæ‰§è¡Œã€‚
    """.trimIndent()
}

// æ‰©å±•å‡½æ•°ï¼šå­—ç¬¦ä¸²é‡å¤
operator fun String.times(n: Int): String = this.repeat(n)
