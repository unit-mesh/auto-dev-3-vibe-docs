#!/usr/bin/env kotlin

/**
 * æ‰‹åŠ¨JVMæµ‹è¯•è„šæœ¬ - éªŒè¯ Custom OpenAI (GLM) æ”¯æŒ
 * 
 * è¿™ä¸ªè„šæœ¬éªŒè¯ä»¥ä¸‹åŠŸèƒ½ï¼š
 * 1. CUSTOM_OPENAI_BASE provider type æ˜¯å¦æ­£ç¡®å®šä¹‰
 * 2. ModelConfig éªŒè¯é€»è¾‘æ˜¯å¦æ­£ç¡®
 * 3. ExecutorFactory æ˜¯å¦èƒ½åˆ›å»ºæ­£ç¡®çš„ executor
 * 4. ModelRegistry æ˜¯å¦èƒ½åˆ›å»ºæ­£ç¡®çš„æ¨¡å‹
 */

@file:DependsOn("org.jetbrains.kotlinx:kotlinx-coroutines-core:1.7.3")

import cc.unitmesh.llm.*
import cc.unitmesh.llm.clients.CustomOpenAILLMClient
import ai.koog.prompt.llm.LLMProvider

fun main() {
    println("ğŸ§ª JVM å¹³å°æµ‹è¯• - Custom OpenAI (GLM) æ”¯æŒ")
    println("=" * 60)
    
    var passed = 0
    var failed = 0
    
    // Test 1: LLMProviderType æšä¸¾
    println("\nğŸ“‹ Test 1: LLMProviderType.CUSTOM_OPENAI_BASE å­˜åœ¨æ€§æ£€æŸ¥")
    try {
        val provider = LLMProviderType.CUSTOM_OPENAI_BASE
        println("âœ… CUSTOM_OPENAI_BASE provider type å·²å®šä¹‰")
        println("   - displayName: ${provider.displayName}")
        passed++
    } catch (e: Exception) {
        println("âŒ CUSTOM_OPENAI_BASE provider type æœªæ‰¾åˆ°: ${e.message}")
        failed++
    }
    
    // Test 2: ModelConfig éªŒè¯ - æœ‰æ•ˆé…ç½®
    println("\nğŸ“‹ Test 2: ModelConfig éªŒè¯ - æœ‰æ•ˆçš„ GLM é…ç½®")
    try {
        val validConfig = ModelConfig(
            provider = LLMProviderType.CUSTOM_OPENAI_BASE,
            modelName = "glm-4-plus",
            apiKey = "7145ac1bf6474f2783e8b4d52b335ab0.gfq0BBvvFy04iwTb",
            baseUrl = "https://open.bigmodel.cn/api/paas/v4",
            temperature = 0.7,
            maxTokens = 8192
        )
        
        if (validConfig.isValid()) {
            println("âœ… æœ‰æ•ˆçš„ GLM é…ç½®é€šè¿‡éªŒè¯")
            println("   - provider: ${validConfig.provider}")
            println("   - modelName: ${validConfig.modelName}")
            println("   - baseUrl: ${validConfig.baseUrl}")
            passed++
        } else {
            println("âŒ æœ‰æ•ˆé…ç½®æœªé€šè¿‡éªŒè¯")
            failed++
        }
    } catch (e: Exception) {
        println("âŒ é…ç½®éªŒè¯å¤±è´¥: ${e.message}")
        failed++
    }
    
    // Test 3: ModelConfig éªŒè¯ - ç¼ºå°‘ baseUrl
    println("\nğŸ“‹ Test 3: ModelConfig éªŒè¯ - ç¼ºå°‘ baseUrl åº”å¤±è´¥")
    try {
        val invalidConfig = ModelConfig(
            provider = LLMProviderType.CUSTOM_OPENAI_BASE,
            modelName = "glm-4-plus",
            apiKey = "test-key",
            baseUrl = ""
        )
        
        if (!invalidConfig.isValid()) {
            println("âœ… ç¼ºå°‘ baseUrl çš„é…ç½®æ­£ç¡®å¤±è´¥")
            passed++
        } else {
            println("âŒ ç¼ºå°‘ baseUrl çš„é…ç½®åº”è¯¥å¤±è´¥ä½†é€šè¿‡äº†")
            failed++
        }
    } catch (e: Exception) {
        println("âŒ æµ‹è¯•å¼‚å¸¸: ${e.message}")
        failed++
    }
    
    // Test 4: ModelConfig éªŒè¯ - ç¼ºå°‘ apiKey
    println("\nğŸ“‹ Test 4: ModelConfig éªŒè¯ - ç¼ºå°‘ apiKey åº”å¤±è´¥")
    try {
        val invalidConfig = ModelConfig(
            provider = LLMProviderType.CUSTOM_OPENAI_BASE,
            modelName = "glm-4-plus",
            apiKey = "",
            baseUrl = "https://open.bigmodel.cn/api/paas/v4"
        )
        
        if (!invalidConfig.isValid()) {
            println("âœ… ç¼ºå°‘ apiKey çš„é…ç½®æ­£ç¡®å¤±è´¥")
            passed++
        } else {
            println("âŒ ç¼ºå°‘ apiKey çš„é…ç½®åº”è¯¥å¤±è´¥ä½†é€šè¿‡äº†")
            failed++
        }
    } catch (e: Exception) {
        println("âŒ æµ‹è¯•å¼‚å¸¸: ${e.message}")
        failed++
    }
    
    // Test 5: ExecutorFactory åˆ›å»º
    println("\nğŸ“‹ Test 5: ExecutorFactory èƒ½åˆ›å»º Custom OpenAI executor")
    try {
        val config = ModelConfig(
            provider = LLMProviderType.CUSTOM_OPENAI_BASE,
            modelName = "glm-4-plus",
            apiKey = "test-key",
            baseUrl = "https://open.bigmodel.cn/api/paas/v4"
        )
        
        val executor = ExecutorFactory.create(config)
        println("âœ… ExecutorFactory æˆåŠŸåˆ›å»º executor")
        println("   - executor type: ${executor::class.simpleName}")
        passed++
    } catch (e: Exception) {
        println("âŒ ExecutorFactory åˆ›å»ºå¤±è´¥: ${e.message}")
        e.printStackTrace()
        failed++
    }
    
    // Test 6: ModelRegistry åˆ›å»ºé€šç”¨æ¨¡å‹
    println("\nğŸ“‹ Test 6: ModelRegistry åˆ›å»º Generic Model")
    try {
        val model = ModelRegistry.createGenericModel(
            provider = LLMProviderType.CUSTOM_OPENAI_BASE,
            modelName = "glm-4-plus",
            contextLength = 128000L
        )
        
        println("âœ… ModelRegistry æˆåŠŸåˆ›å»ºæ¨¡å‹")
        println("   - model id: ${model.id}")
        println("   - provider: ${model.provider}")
        println("   - contextLength: ${model.contextLength}")
        
        if (model.provider == LLMProvider.OpenAI && model.id == "glm-4-plus") {
            passed++
        } else {
            println("âŒ æ¨¡å‹å±æ€§ä¸æ­£ç¡®")
            failed++
        }
    } catch (e: Exception) {
        println("âŒ ModelRegistry åˆ›å»ºå¤±è´¥: ${e.message}")
        failed++
    }
    
    // Test 7: CustomOpenAILLMClient åˆ›å»º
    println("\nğŸ“‹ Test 7: CustomOpenAILLMClient å®ä¾‹åŒ–")
    try {
        val client = CustomOpenAILLMClient(
            apiKey = "test-api-key",
            baseUrl = "https://open.bigmodel.cn/api/paas/v4",
            chatCompletionsPath = "chat/completions"
        )
        
        println("âœ… CustomOpenAILLMClient æˆåŠŸå®ä¾‹åŒ–")
        println("   - provider: ${client.llmProvider()}")
        
        if (client.llmProvider() == LLMProvider.OpenAI) {
            passed++
        } else {
            println("âŒ Client provider åº”è¯¥æ˜¯ OpenAI")
            failed++
        }
    } catch (e: Exception) {
        println("âŒ CustomOpenAILLMClient å®ä¾‹åŒ–å¤±è´¥: ${e.message}")
        e.printStackTrace()
        failed++
    }
    
    // Test 8: LLMService éªŒè¯æ— æ•ˆé…ç½®
    println("\nğŸ“‹ Test 8: LLMService åº”æ‹’ç»æ— æ•ˆé…ç½®")
    try {
        val invalidConfig = ModelConfig(
            provider = LLMProviderType.CUSTOM_OPENAI_BASE,
            modelName = "glm-4-plus",
            apiKey = "",
            baseUrl = ""
        )
        
        try {
            LLMService.create(invalidConfig)
            println("âŒ LLMService åº”è¯¥æ‹’ç»æ— æ•ˆé…ç½®")
            failed++
        } catch (e: IllegalArgumentException) {
            println("âœ… LLMService æ­£ç¡®æ‹’ç»æ— æ•ˆé…ç½®")
            println("   - error message: ${e.message}")
            passed++
        }
    } catch (e: Exception) {
        println("âŒ æµ‹è¯•å¼‚å¸¸: ${e.message}")
        failed++
    }
    
    // æ€»ç»“
    println("\n" + "=" * 60)
    println("ğŸ“Š æµ‹è¯•æ€»ç»“")
    println("=" * 60)
    println("âœ… é€šè¿‡: $passed")
    println("âŒ å¤±è´¥: $failed")
    println("ğŸ“ˆ é€šè¿‡ç‡: ${(passed * 100.0 / (passed + failed)).toInt()}%")
    
    if (failed == 0) {
        println("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Custom OpenAI (GLM) æ”¯æŒåœ¨ JVM å¹³å°ä¸Šæ­£å¸¸å·¥ä½œï¼")
    } else {
        println("\nâš ï¸  æœ‰ $failed ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤")
        System.exit(1)
    }
}

operator fun String.times(count: Int) = this.repeat(count)

